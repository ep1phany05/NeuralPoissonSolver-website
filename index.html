
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Neural Poisson Solver</title>

    <!-- <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1"> -->

    <!-- <meta property="og:image" content="https://jonbarron.info/mipnerf360/img/gardenvase.jpg"> -->
    <!-- <meta property="og:image:type" content="image/png"> -->
    <!-- <meta property="og:image:width" content="1296"> -->
    <!-- <meta property="og:image:height" content="840"> -->
    <!-- <meta property="og:type" content="website" /> -->
    <!-- <meta property="og:url" content="https://jonbarron.info/mipnerf360/"/> -->
    <!-- <meta property="og:title" content="Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields" /> -->
    <!-- <meta property="og:description" content="Though neural radiance fields (NeRF) have demonstrated impressive view synthesis results on objects and small bounded regions of space, they struggle on 'unbounded' scenes, where the camera may point in any direction and content may exist at any distance. In this setting, existing NeRF-like models often produce blurry or low-resolution renderings (due to the unbalanced detail and scale of nearby and distant objects), are slow to train, and may exhibit artifacts due to the inherent ambiguity of the task of reconstructing a large scene from a small set of images. We present an extension of mip-NeRF (a NeRF variant that addresses sampling and aliasing) that uses a non-linear scene parameterization, online distillation, and a novel distortion-based regularizer to overcome the challenges presented by unbounded scenes. Our model, which we dub 'mip-NeRF 360' as we target scenes in which the camera rotates 360 degrees around a point, reduces mean-squared error by 54% compared to mip-NeRF, and is able to produce realistic synthesized views and detailed depth maps for highly intricate, unbounded real-world scenes." />

    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields" />
    <meta name="twitter:description" content="Though neural radiance fields (NeRF) have demonstrated impressive view synthesis results on objects and small bounded regions of space, they struggle on 'unbounded' scenes, where the camera may point in any direction and content may exist at any distance. In this setting, existing NeRF-like models often produce blurry or low-resolution renderings (due to the unbalanced detail and scale of nearby and distant objects), are slow to train, and may exhibit artifacts due to the inherent ambiguity of the task of reconstructing a large scene from a small set of images. We present an extension of mip-NeRF (a NeRF variant that addresses sampling and aliasing) that uses a non-linear scene parameterization, online distillation, and a novel distortion-based regularizer to overcome the challenges presented by unbounded scenes. Our model, which we dub 'mip-NeRF 360' as we target scenes in which the camera rotates 360 degrees around a point, reduces mean-squared error by 54% compared to mip-NeRF, and is able to produce realistic synthesized views and detailed depth maps for highly intricate, unbounded real-world scenes." />
    <meta name="twitter:image" content="https://jonbarron.info/mipnerf360/img/gardenvase.jpg" /> -->


    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ’«</text></svg>">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">
    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>Neural Poisson Solver</b>:  <br> A Universal and Continuous Framework <br> for Natural Signal Blending </br>
                <small>
                    ECCV 2024
                </small>

            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://ep1phany05.github.io/">
                          Delong Wu
                        </a>
                        </br>Nanjing University
                    </li>
                    <li>
                        <a href="https://pakfa.github.io/zhuhao_photo.github.io/">
                          Hao Zhu
                        </a>
                        </br>Nanjing University
                    </li>
                    <li>
                        <a href="https://qzhang-cv.github.io/">
                          Qi Zhang
                        </a>
                        </br>Tencent AI Lab
                    </li><br>
                    <li>
                        <a>
                          You Li
                        </a>
                        </br>China Astronaut Research and Training Center
                    </li>
                    <li>
                        <a href="https://cite.nju.edu.cn/People/Faculty/20190621/i5054.html">
                          Xun Cao
                        </a>
                        </br>Nanjing University
                    </li>
                    <li>
                        <a href="https://vision.nju.edu.cn/fc/d3/c29470a457939/page.htm">
                          Zhan Ma
                        </a>
                        </br>Nanjing University
                    </li>
                </ul>

            </div>

        </div>

        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="">
                            <image src="img/paper-cover.jpg" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/ep1phany05/NeuralPoissonSolver">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <img src="img/full_res_hashmapping.png" height="250px">
						</div>
            <div class="col-md-8 col-md-offset-2">
							<p class="text-center">
                                Pipeline of the DINER.
							</p>
						</div>
        </div>




        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    Implicit Neural Representation (INR) has become a popular method for representing visual signals (e.g., 2D images and 3D scenes),
                    demonstrating promising results in various downstream applications. Given its potential as a medium for visual signals,
                    exploring the development of a neural blending method that utilizes INRs is a natural progression. Neural blending
                    involves merging two INRs to create a new INR that encapsulates information from both original representations.
                    A direct approach involves applying traditional image editing methods to the INR rendering process. However,
                    this method often results in blending distortions, artifacts, and color shifts, primarily due to the discretization
                    of the underlying pixel grid and the introduction of boundary conditions for solving variational problems. To tackle
                    this issue, we introduce the Neural Poisson Solver, a plug-and-play and universally applicable framework across
                    different signal dimensions for blending visual signals represented by INRs. Our Neural Poisson Solver offers a
                    variational problem-solving approach based on the continuous Poisson equation, demonstrating exceptional performance
                    across various domains. Specifically, we propose a gradient-guided neural solver to represent the solution process
                    of the variational problem, refining the target signal to achieve natural blending results. We also develop a Poisson
                    equation-based loss and optimization scheme to train our solver, ensuring it effectively blends the input INR scenes
                    while preserving their inherent structure and semantic content. The lack of dependence on additional prior knowledge
                    makes our method easily adaptable to various task categories, highlighting its versatility. Comprehensive experimental
                    results validate the robustness of our approach across multiple dimensions and blending tasks.
                </p>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <img src="img/freq_cmp_baboon.png" height="330px">
						</div>
            <div class="col-md-8 col-md-offset-2">
							<p class="text-center">
                                Comparisons of the existing INR and DINER for representing Baboon with different arrangements. From top to bottom, pixels in the Baboon are arranged in different geometric orders, while the histogram is not changed (the right-bottom panel in (a)). From left to right, (a) and (b) refer to the ground truth image and its Fourier spectrum. (c) contains results by an MLP with positional encoding (PE+MLP) [38] at a size of 2 Ã— 64. (d) refers to the hash-mapped coordinates. (e) refers to the results of the DINER that uses the same-size MLP as (c). (f) refers to the learned INR in DINER by directly feeding grid coordinates to the trained MLP (see Sec. 4.1 for more details).(g) is the Fourier spectrum of (f). (g) shares the same scale bar with (b).
							</p>
						</div>
        </div>




        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Experiments
                </h3>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <img src="img/res_lensless.png" height="380px">
						</div>
            <div class="col-md-8 col-md-offset-2">
							<p class="text-center">
                                Comparisons on lensless imaging.
							</p>
						</div>
        </div>

        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <img src="img/RI_IDT.png" height="420px">
						</div>
            <div class="col-md-8 col-md-offset-2">
							<p class="text-center">
                                Comparisons on 3D refractive index recovery. DINER
takes less training time and could reconstruct more surface details
of the Granulocyte.
							</p>
						</div>
        </div>


        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://youtube.com/embed/zBSH-k9GbV4" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div> -->

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Learned INRs
                </h3>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <img src="img/hash_feature.png" height="820px">
						</div>
            <div class="col-md-8 col-md-offset-2">
							<p class="text-center">
                                Hash features.
							</p>
						</div>
        </div>


            
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@inproceedings{xie2023diner,
    author = {Xie, Shaowen and Zhu, Hao and Liu, Zhen and Zhang, Qi and Zhou, You and Cao, Xun and Ma, Zhan},
    title = {DINER: Disorder-Invariant Implicit Neural Representation},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    year = {2023}
}</textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                <!-- Thanks to Ricardo Martin-Brualla and David Salesin for their comments on the text, and to George Drettakis and Georgios Kopanas for graciously assisting us with our baseline evaluation.
                    <br> -->
                The website template was borrowed from <a href="http://mgharbi.com/">MichaÃ«l Gharbi</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
